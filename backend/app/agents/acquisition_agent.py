"""
Acquisition Agent for web-based data collection.

This agent is responsible for acquiring new training data from
the web based on the search strategy generated by AnalysisAgent.
"""

from typing import Any

from app.agents.base_agent import BaseAgent
from app.agents.state import AgentState
from app.tools.web_crawler import WebCrawler
from app.tools.subprocess_executor import SubprocessExecutor
from app.tools.supabase_client import SupabaseClient
from app.core.config import settings
from app.core.logging_config import get_logger

logger = get_logger(__name__)


class AcquisitionAgent(BaseAgent):
    """
    Agent for data acquisition.
    
    This agent:
    1. Takes search queries from AnalysisAgent
    2. Crawls web for relevant images
    3. Runs pseudo-labeling on acquired images
    4. Prepares dataset for training
    """

    def __init__(
        self,
        web_crawler: WebCrawler,
        subprocess_executor: SubprocessExecutor,
        supabase_client: SupabaseClient
    ) -> None:
        """
        Initialize the Acquisition Agent.
        
        Args:
            web_crawler: Web crawler for image acquisition
            subprocess_executor: Executor for running pseudo-labeling
            supabase_client: Client for database operations
        """
        super().__init__(agent_name="AcquisitionAgent")
        self.web_crawler = web_crawler
        self.subprocess_executor = subprocess_executor
        self.supabase_client = supabase_client

    async def execute(self, context: dict[str, Any]) -> dict[str, Any]:
        """
        Execute data acquisition and pseudo-labeling.
        
        Args:
            context: Must contain:
                - job_id: str
                - search_queries: list[str]
                - model_path: str (for pseudo-labeling)
                
        Returns:
            Dictionary containing:
                - acquired_images: list[str]
                - pseudo_labels: list[dict]
                - dataset_ready: bool
        """
        self._log_execution_start()
        
        try:
            job_id = context["job_id"]
            search_queries = context.get("search_queries", [])
            model_path = context.get("model_path", "yolov5s.pt")
            
            if not search_queries:
                raise ValueError("No search queries found in context")
            
            logger.info(
                f"Acquiring data with {len(search_queries)} queries",
                extra={"job_id": job_id}
            )
            
            # Step 1: Crawl images from web
            acquired_images = await self.web_crawler.crawl_images(
                queries=search_queries,
                limit=10,  # Images per query
                job_id=job_id
            )
            
            if not acquired_images:
                logger.warning("No images acquired from web")
                return {
                    "acquired_images": [],
                    "pseudo_labels": [],
                    "dataset_ready": False,
                }
            
            logger.info(f"Acquired {len(acquired_images)} images from web")
            
            # Step 2: Run pseudo-labeling on acquired images
            pseudo_labels = await self._run_pseudo_labeling(
                images=acquired_images,
                model_path=model_path,
                job_id=job_id
            )
            
            # Step 3: Filter high-quality pseudo labels
            filtered_labels = self._filter_pseudo_labels(
                pseudo_labels,
                confidence_threshold=settings.pseudo_label_confidence_threshold
            )
            
            logger.info(
                f"Generated {len(filtered_labels)} high-quality pseudo labels"
            )
            
            self._log_execution_end(
                f"Acquired {len(acquired_images)} images, "
                f"{len(filtered_labels)} labeled"
            )
            
            return {
                "acquired_images": acquired_images,
                "pseudo_labels": filtered_labels,
                "total_acquired": len(acquired_images),
                "high_quality_labels": len(filtered_labels),
                "dataset_ready": len(filtered_labels) > 0,
            }
            
        except Exception as e:
            self._log_error(e)
            raise

    async def _run_pseudo_labeling(
        self,
        images: list[str],
        model_path: str,
        job_id: str
    ) -> list[dict[str, Any]]:
        """
        Run pseudo-labeling on acquired images.
        
        Args:
            images: List of image URLs/paths
            model_path: Path to model for pseudo-labeling
            job_id: Job identifier
            
        Returns:
            List of pseudo labels
        """
        # Download images if they are URLs
        local_image_paths = await self._download_images(images, job_id)
        
        if not local_image_paths:
            return []
        
        # Run inference for pseudo-labeling
        output_path = f"/tmp/pseudo_labels/{job_id}"
        
        result = await self.subprocess_executor.run_yolo_predict(
            model_path=model_path,
            source_path=",".join(local_image_paths),
            output_path=output_path,
            conf_threshold=settings.pseudo_label_confidence_threshold,
            iou_threshold=0.45,
        )
        
        # Parse pseudo labels
        # (Similar to InferenceAgent's parsing logic)
        pseudo_labels = []
        # ... implementation details ...
        
        return pseudo_labels

    async def _download_images(
        self,
        image_urls: list[str],
        job_id: str
    ) -> list[str]:
        """
        Download images from URLs to local storage.
        
        Args:
            image_urls: List of image URLs
            job_id: Job identifier
            
        Returns:
            List of local file paths
        """
        local_paths = []
        # Implementation would download images
        # For now, assume images are already accessible
        return image_urls

    def _filter_pseudo_labels(
        self,
        labels: list[dict[str, Any]],
        confidence_threshold: float
    ) -> list[dict[str, Any]]:
        """
        Filter pseudo labels based on confidence threshold.
        
        This implements quality control for semi-supervised learning.
        
        Args:
            labels: List of pseudo label dictionaries
            confidence_threshold: Minimum confidence threshold
            
        Returns:
            Filtered list of high-quality labels
        """
        filtered = []
        
        for label in labels:
            detections = label.get("detections", [])
            high_conf_detections = [
                det for det in detections
                if det.get("confidence", 0.0) >= confidence_threshold
            ]
            
            if high_conf_detections:
                filtered.append({
                    **label,
                    "detections": high_conf_detections
                })
        
        return filtered


# ==================== LangGraph Node Functions ====================

async def hunting_node(state: AgentState) -> AgentState:
    """
    LangGraph 节点函数：数据获取节点（hunter）。
    
    此函数作为 LangGraph 工作流中的 hunter 节点，负责：
    1. 根据分析结果提取的关键词进行网络爬取
    2. 上传获取的图像到 Supabase
    3. 更新状态中的图像 URL 列表
    
    Args:
        state: AgentState 状态字典
        
    Returns:
        更新后的 AgentState
    """
    logger.info("执行数据获取节点 (hunter)")
    
    try:
        # 初始化工具
        supabase_client = SupabaseClient()
        web_crawler = WebCrawler(supabase_client)
        
        # 获取搜索关键词
        search_keywords = state.get("search_keywords", [])
        if not search_keywords:
            # 尝试从其他字段获取
            search_keywords = state.get("search_queries", [])
        
        if not search_keywords:
            error_msg = "未找到搜索关键词"
            logger.warning(error_msg)
            state["errors"] = state.get("errors", []) + [error_msg]
            return state
        
        # 获取项目 ID 或任务 ID
        project_id = state.get("project_id", "")
        job_id = state.get("job_id", project_id)
        
        # 爬取图像
        crawled_images = await web_crawler.crawl_images(
            queries=search_keywords,
            limit=10,  # 每个查询最多 10 张图像
            job_id=job_id
        )
        
        # 更新状态
        current_images = state.get("image_urls", [])
        state["image_urls"] = current_images + crawled_images
        state["acquired_images"] = crawled_images
        state["crawled_count"] = len(crawled_images)
        
        logger.info(
            f"数据获取完成: 爬取 {len(crawled_images)} 张图像"
        )
        
        return state
        
    except Exception as e:
        error_msg = f"数据获取节点执行失败: {str(e)}"
        logger.error(error_msg, exc_info=True)
        state["errors"] = state.get("errors", []) + [error_msg]
        return state

