#!/usr/bin/env python
"""
å‘ Supabase æ’å…¥æµ‹è¯•æ•°æ®è„šæœ¬ã€‚

æ­¤è„šæœ¬ä¼šï¼š
1. åˆ›å»ºå¤šä¸ªæµ‹è¯•ä»»åŠ¡ï¼ˆjobsï¼‰
2. çˆ¬å–ä¸€äº›é¥æ„Ÿå½±åƒå›¾ç‰‡
3. ä¸Šä¼ å›¾ç‰‡åˆ° Supabase Storage
4. åˆ›å»ºæ¨ç†ç»“æœè®°å½•ï¼ˆinference_resultsï¼‰
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime, timedelta
import json
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.db.supabase_init import get_supabase_client
from app.tools.supabase_client import SupabaseClient
from app.core.logging_config import get_logger

logger = get_logger(__name__)


async def crawl_sample_images(max_images: int = 10) -> list[dict]:
    """
    çˆ¬å–ä¸€äº›ç¤ºä¾‹é¥æ„Ÿå½±åƒå›¾ç‰‡ã€‚
    
    Args:
        max_images: æœ€å¤§å›¾ç‰‡æ•°é‡
        
    Returns:
        å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å« URL å’Œå…ƒæ•°æ®
    """
    try:
        from playwright.async_api import async_playwright
        import httpx
        
        print(f"\nğŸ” å¼€å§‹çˆ¬å– {max_images} å¼ é¥æ„Ÿå½±åƒå›¾ç‰‡...")
        
        image_data = []
        search_queries = ["é¥æ„Ÿå½±åƒ", "å«æ˜Ÿå›¾åƒ", "èˆªæ‹å›¾åƒ"]
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.set_extra_http_headers({
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            })
            
            images_collected = 0
            
            for query in search_queries:
                if images_collected >= max_images:
                    break
                    
                print(f"   æœç´¢å…³é”®è¯: {query}")
                search_url = f"https://www.bing.com/images/search?q={query}&first=1"
                
                try:
                    await page.goto(search_url, wait_until="networkidle", timeout=10000)
                    await page.wait_for_selector("img.mimg", timeout=5000)
                    
                    image_elements = await page.query_selector_all("img.mimg")
                    
                    for img in image_elements:
                        if images_collected >= max_images:
                            break
                            
                        src = await img.get_attribute("src")
                        if src and (src.startswith("http") or src.startswith("https")):
                            # éªŒè¯å›¾ç‰‡æ˜¯å¦å¯è®¿é—®
                            try:
                                async with httpx.AsyncClient(timeout=5.0) as client:
                                    response = await client.head(src)
                                    if response.status_code == 200:
                                        image_data.append({
                                            "url": src,
                                            "query": query,
                                            "index": images_collected + 1
                                        })
                                        images_collected += 1
                                        print(f"   âœ… æ‰¾åˆ°å›¾ç‰‡ {images_collected}/{max_images}")
                            except:
                                continue
                                
                except Exception as e:
                    print(f"   âš ï¸  æœç´¢ {query} æ—¶å‡ºé”™: {e}")
                    continue
            
            await browser.close()
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(image_data)} å¼ å›¾ç‰‡\n")
        return image_data
        
    except ImportError:
        print("âš ï¸  Playwright æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå›¾ç‰‡æ•°æ®")
        # è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return [
            {"url": f"https://example.com/satellite_{i}.jpg", "query": "é¥æ„Ÿå½±åƒ", "index": i}
            for i in range(1, max_images + 1)
        ]
    except Exception as e:
        print(f"âš ï¸  çˆ¬å–å›¾ç‰‡å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return [
            {"url": f"https://example.com/satellite_{i}.jpg", "query": "é¥æ„Ÿå½±åƒ", "index": i}
            for i in range(1, max_images + 1)
        ]


async def upload_image_to_storage(
    supabase_client: SupabaseClient,
    image_url: str,
    job_id: str,
    image_index: int
) -> str:
    """
    ä¸Šä¼ å›¾ç‰‡åˆ° Supabase Storageã€‚
    
    Args:
        supabase_client: Supabase å®¢æˆ·ç«¯
        image_url: å›¾ç‰‡ URL
        job_id: ä»»åŠ¡ ID
        image_index: å›¾ç‰‡ç´¢å¼•
        
    Returns:
        å­˜å‚¨è·¯å¾„
    """
    try:
        import httpx
        
        # ä¸‹è½½å›¾ç‰‡
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(image_url, follow_redirects=True)
            if response.status_code != 200:
                raise Exception(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {response.status_code}")
            
            image_data = response.content
            
        # ç¡®å®šæ–‡ä»¶æ‰©å±•å
        extension = ".jpg"
        if ".png" in image_url.lower():
            extension = ".png"
        elif ".webp" in image_url.lower():
            extension = ".webp"
        
        # æ„å»ºå­˜å‚¨è·¯å¾„
        storage_path = f"{job_id}/images/image_{image_index:03d}{extension}"
        
        # ä¸Šä¼ åˆ° Supabase Storage
        try:
            supabase_client.client.storage.from_("images").upload(
                path=storage_path,
                file=image_data,
                file_options={"content-type": f"image/{extension[1:]}"}
            )
            print(f"   âœ… ä¸Šä¼ å›¾ç‰‡: {storage_path}")
            return storage_path
        except Exception as e:
            # å¦‚æœ bucket ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ 
            print(f"   âš ï¸  ä¸Šä¼ å¤±è´¥ï¼ˆå¯èƒ½ bucket ä¸å­˜åœ¨ï¼‰: {e}")
            return storage_path
            
    except Exception as e:
        print(f"   âš ï¸  å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
        return f"{job_id}/images/image_{image_index:03d}.jpg"


async def create_test_jobs(supabase_client: SupabaseClient, num_jobs: int = 5):
    """åˆ›å»ºæµ‹è¯•ä»»åŠ¡ã€‚"""
    print(f"\nğŸ“ åˆ›å»º {num_jobs} ä¸ªæµ‹è¯•ä»»åŠ¡...")
    
    job_statuses = [
        "UPLOAD",
        "INFERENCE",
        "ANALYSIS",
        "ACQUISITION",
        "PSEUDO_LABELING",
        "TRAINING",
        "COMPLETE",
        "FAILED"
    ]
    
    progress_messages = [
        "æ­£åœ¨ä¸Šä¼ ç§å­æ•°æ®...",
        "æ­£åœ¨è¿›è¡Œæ¨ç†åˆ†æ...",
        "æ­£åœ¨åˆ†ææ•°æ®è´¨é‡...",
        "æ­£åœ¨é‡‡é›†æ–°æ ·æœ¬...",
        "æ­£åœ¨ç”Ÿæˆä¼ªæ ‡ç­¾...",
        "æ­£åœ¨è®­ç»ƒæ¨¡å‹...",
        "ä»»åŠ¡å·²å®Œæˆ",
        "ä»»åŠ¡æ‰§è¡Œå¤±è´¥"
    ]
    
    jobs = []
    
    for i in range(1, num_jobs + 1):
        status = random.choice(job_statuses)
        status_index = job_statuses.index(status)
        
        job_id = f"test_job_{datetime.now().strftime('%Y%m%d')}_{i:03d}"
        
        # åˆ›å»ºæ—¶é—´ï¼šéšæœºåˆ†å¸ƒåœ¨è¿‡å»7å¤©å†…
        days_ago = random.randint(0, 7)
        created_at = (datetime.now() - timedelta(days=days_ago)).isoformat()
        updated_at = (datetime.now() - timedelta(hours=random.randint(0, 24))).isoformat()
        
        metadata = {
            "project_name": f"æµ‹è¯•é¡¹ç›® {i}",
            "description": f"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é¡¹ç›®ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½",
            "model_type": random.choice(["YOLOv8", "YOLOv9", "YOLOv10"]),
            "dataset_size": random.randint(100, 1000),
            "accuracy": round(random.uniform(0.65, 0.95), 3) if status == "COMPLETE" else None,
            "epochs": random.randint(10, 100) if status in ["TRAINING", "COMPLETE"] else None,
            "created_by": "test_script"
        }
        
        job_data = {
            "job_id": job_id,
            "status": status,
            "progress_message": progress_messages[status_index] if status_index < len(progress_messages) else None,
            "metadata": metadata,
            "created_at": created_at,
            "updated_at": updated_at
        }
        
        try:
            response = supabase_client.client.table("jobs").insert(job_data).execute()
            jobs.append(response.data[0] if response.data else job_data)
            print(f"   âœ… åˆ›å»ºä»»åŠ¡: {job_id} ({status})")
        except Exception as e:
            print(f"   âš ï¸  åˆ›å»ºä»»åŠ¡å¤±è´¥: {job_id} ({e})")
            # å¦‚æœå·²å­˜åœ¨ï¼Œå°è¯•æ›´æ–°
            try:
                update_data = {k: v for k, v in job_data.items() if k != "job_id"}
                supabase_client.client.table("jobs").update(update_data).eq("job_id", job_id).execute()
                print(f"   âœ… æ›´æ–°ä»»åŠ¡: {job_id}")
            except:
                pass
    
    print(f"\nâœ… æˆåŠŸåˆ›å»º/æ›´æ–° {len(jobs)} ä¸ªä»»åŠ¡\n")
    return jobs


async def create_test_inference_results(
    supabase_client: SupabaseClient,
    jobs: list[dict],
    images_per_job: int = 3
):
    """ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ¨ç†ç»“æœã€‚"""
    print(f"\nğŸ”¬ ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»º {images_per_job} æ¡æ¨ç†ç»“æœ...")
    
    # çˆ¬å–ä¸€äº›å›¾ç‰‡
    all_images = await crawl_sample_images(max_images=len(jobs) * images_per_job)
    
    object_classes = ["ship", "airplane", "vehicle", "building", "road", "bridge", "port"]
    
    total_created = 0
    
    for job_idx, job in enumerate(jobs):
        job_id = job["job_id"]
        print(f"\n   å¤„ç†ä»»åŠ¡: {job_id}")
        
        # ä¸ºè¿™ä¸ªä»»åŠ¡åˆ†é…å›¾ç‰‡
        start_idx = job_idx * images_per_job
        job_images = all_images[start_idx:start_idx + images_per_job]
        
        for img_idx, img_info in enumerate(job_images, 1):
            # ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
            num_detections = random.randint(1, 5)
            predictions = []
            
            for det_idx in range(num_detections):
                predictions.append({
                    "class": random.choice(object_classes),
                    "confidence": round(random.uniform(0.5, 0.95), 3),
                    "bbox": [
                        round(random.uniform(0, 0.7), 2),  # x
                        round(random.uniform(0, 0.7), 2),  # y
                        round(random.uniform(0.2, 0.3), 2),  # width
                        round(random.uniform(0.2, 0.3), 2)   # height
                    ]
                })
            
            # å°è¯•ä¸Šä¼ å›¾ç‰‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                image_path = await upload_image_to_storage(
                    supabase_client,
                    img_info["url"],
                    job_id,
                    img_idx
                )
            except:
                image_path = f"{job_id}/images/image_{img_idx:03d}.jpg"
            
            # åˆ›å»ºæ¨ç†ç»“æœè®°å½•
            inference_data = {
                "job_id": job_id,
                "image_path": image_path,
                "predictions": predictions,
                "created_at": (datetime.now() - timedelta(hours=random.randint(0, 24))).isoformat()
            }
            
            try:
                response = supabase_client.client.table("inference_results").insert(inference_data).execute()
                total_created += 1
                print(f"   âœ… åˆ›å»ºæ¨ç†ç»“æœ {img_idx}/{images_per_job}: {len(predictions)} ä¸ªæ£€æµ‹")
            except Exception as e:
                print(f"   âš ï¸  åˆ›å»ºæ¨ç†ç»“æœå¤±è´¥: {e}")
    
    print(f"\nâœ… æˆåŠŸåˆ›å»º {total_created} æ¡æ¨ç†ç»“æœè®°å½•\n")


async def main():
    """ä¸»å‡½æ•°ã€‚"""
    print("\n" + "="*70)
    print("Supabase æµ‹è¯•æ•°æ®æ’å…¥è„šæœ¬")
    print("="*70)
    
    try:
        # åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
        print("\n1ï¸âƒ£  åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯...")
        supabase_client = SupabaseClient()
        print("   âœ… Supabase å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\n")
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        print("2ï¸âƒ£  åˆ›å»ºæµ‹è¯•ä»»åŠ¡...")
        jobs = await create_test_jobs(supabase_client, num_jobs=5)
        
        # åˆ›å»ºæ¨ç†ç»“æœ
        print("3ï¸âƒ£  åˆ›å»ºæ¨ç†ç»“æœ...")
        await create_test_inference_results(supabase_client, jobs, images_per_job=3)
        
        # éªŒè¯æ•°æ®
        print("4ï¸âƒ£  éªŒè¯æ•°æ®...")
        jobs_count = supabase_client.client.table("jobs").select("*", count="exact").execute()
        results_count = supabase_client.client.table("inference_results").select("*", count="exact").execute()
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   - Jobs è¡¨: {jobs_count.count if hasattr(jobs_count, 'count') else len(jobs_count.data)} æ¡è®°å½•")
        print(f"   - Inference Results è¡¨: {results_count.count if hasattr(results_count, 'count') else len(results_count.data)} æ¡è®°å½•")
        
        print("\n" + "="*70)
        print("âœ… æµ‹è¯•æ•°æ®æ’å…¥å®Œæˆï¼")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

