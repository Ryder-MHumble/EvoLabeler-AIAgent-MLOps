"""
Playwright ç½‘ç»œçˆ¬è™«æµ‹è¯•ã€‚

æ­¤æµ‹è¯•æ–‡ä»¶ç”¨äºéªŒè¯ WebCrawler çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. æµè§ˆå™¨å¯åŠ¨å’Œé¡µé¢å¯¼èˆª
2. å›¾ç‰‡æœç´¢å’Œä¸‹è½½
3. Supabase ä¸Šä¼ 
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.tools.web_crawler import WebCrawler
from app.tools.supabase_client import SupabaseClient
from app.core.logging_config import setup_logging, get_logger

# è®¾ç½®æ—¥å¿—
setup_logging()
logger = get_logger(__name__)


async def test_basic_crawler():
    """
    æµ‹è¯•åŸºç¡€çˆ¬è™«åŠŸèƒ½ã€‚
    
    è¿™ä¸ªæµ‹è¯•ä¼šï¼š
    1. å¯åŠ¨ Playwright æµè§ˆå™¨
    2. æœç´¢æŒ‡å®šå…³é”®è¯çš„å›¾ç‰‡
    3. ä¸‹è½½å‰å‡ å¼ å›¾ç‰‡
    4. æ‰“å°ç»“æœ
    """
    print("\n" + "="*60)
    print("æµ‹è¯• 1: åŸºç¡€çˆ¬è™«åŠŸèƒ½ï¼ˆä¸ä¸Šä¼ åˆ° Supabaseï¼‰")
    print("="*60 + "\n")
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä¸è¿æ¥ Supabaseï¼‰
        from playwright.async_api import async_playwright
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "é¥æ„Ÿå½±åƒ å«æ˜Ÿ",
            "satellite imagery"
        ]
        
        print(f"ğŸ” æµ‹è¯•æœç´¢å…³é”®è¯: {test_queries}")
        print("="*60)
        
        async with async_playwright() as p:
            # å¯åŠ¨æµè§ˆå™¨
            print("\nğŸ“± æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            browser = await p.chromium.launch(headless=False)  # headless=False å¯ä»¥çœ‹åˆ°æµè§ˆå™¨
            context = await browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/120.0.0.0 Safari/537.36"
            )
            
            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
            # æµ‹è¯•ç¬¬ä¸€ä¸ªæŸ¥è¯¢
            query = test_queries[0]
            print(f"\nğŸ” æ­£åœ¨æœç´¢: {query}")
            
            page = await context.new_page()
            
            try:
                # è®¿é—®å¿…åº”å›¾ç‰‡æœç´¢
                search_url = f"https://www.bing.com/images/search?q={query}"
                print(f"ğŸ“ è®¿é—®: {search_url}")
                
                await page.goto(search_url, wait_until="networkidle", timeout=30000)
                print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                
                # ç­‰å¾…å›¾ç‰‡åŠ è½½
                print("â³ ç­‰å¾…å›¾ç‰‡åŠ è½½...")
                await page.wait_for_selector("img.mimg", timeout=10000)
                print("âœ… å›¾ç‰‡å·²åŠ è½½")
                
                # è·å–å›¾ç‰‡å…ƒç´ 
                image_elements = await page.query_selector_all("img.mimg")
                print(f"\nğŸ“Š æ‰¾åˆ° {len(image_elements)} å¼ å›¾ç‰‡")
                
                # è·å–å‰ 3 å¼ å›¾ç‰‡çš„ä¿¡æ¯
                for i, img_elem in enumerate(image_elements[:3]):
                    try:
                        img_src = await img_elem.get_attribute("src")
                        img_alt = await img_elem.get_attribute("alt")
                        
                        print(f"\nå›¾ç‰‡ {i+1}:")
                        print(f"  URL: {img_src[:80]}..." if img_src and len(img_src) > 80 else f"  URL: {img_src}")
                        print(f"  æè¿°: {img_alt}")
                        
                    except Exception as e:
                        print(f"  âŒ è·å–å›¾ç‰‡ {i+1} ä¿¡æ¯å¤±è´¥: {e}")
                
                # æˆªå›¾ä¿å­˜
                screenshot_path = "/tmp/crawler_test_screenshot.png"
                await page.screenshot(path=screenshot_path)
                print(f"\nğŸ“¸ é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                
            finally:
                await page.close()
            
            await browser.close()
            print("\nâœ… æµè§ˆå™¨å·²å…³é—­")
        
        print("\n" + "="*60)
        print("âœ… æµ‹è¯• 1 å®Œæˆ")
        print("="*60 + "\n")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_crawler_with_download():
    """
    æµ‹è¯•çˆ¬è™«ä¸‹è½½åŠŸèƒ½ã€‚
    
    è¿™ä¸ªæµ‹è¯•ä¼šä¸‹è½½å®é™…çš„å›¾ç‰‡æ–‡ä»¶ã€‚
    """
    print("\n" + "="*60)
    print("æµ‹è¯• 2: å›¾ç‰‡ä¸‹è½½åŠŸèƒ½")
    print("="*60 + "\n")
    
    try:
        from playwright.async_api import async_playwright
        import hashlib
        
        test_query = "remote sensing image"
        download_limit = 2
        
        print(f"ğŸ” æœç´¢å…³é”®è¯: {test_query}")
        print(f"ğŸ“¥ ä¸‹è½½æ•°é‡: {download_limit}")
        print("="*60)
        
        downloaded_files = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()
            
            # æœç´¢å›¾ç‰‡
            search_url = f"https://www.bing.com/images/search?q={test_query}"
            await page.goto(search_url, wait_until="networkidle", timeout=30000)
            await page.wait_for_selector("img.mimg", timeout=10000)
            
            # è·å–å›¾ç‰‡
            image_elements = await page.query_selector_all("img.mimg")
            print(f"âœ… æ‰¾åˆ° {len(image_elements)} å¼ å›¾ç‰‡")
            
            for i, img_elem in enumerate(image_elements[:download_limit]):
                try:
                    img_src = await img_elem.get_attribute("src")
                    
                    if not img_src or img_src.startswith("data:"):
                        continue
                    
                    print(f"\nğŸ“¥ ä¸‹è½½å›¾ç‰‡ {i+1}...")
                    print(f"   URL: {img_src[:60]}...")
                    
                    # ä¸‹è½½å›¾ç‰‡
                    response = await page.goto(img_src, timeout=15000)
                    if response and response.status == 200:
                        image_data = await response.body()
                        
                        if len(image_data) > 1024:  # è‡³å°‘ 1KB
                            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                            file_hash = hashlib.md5(image_data).hexdigest()[:8]
                            file_path = f"/tmp/test_image_{i}_{file_hash}.jpg"
                            
                            with open(file_path, 'wb') as f:
                                f.write(image_data)
                            
                            downloaded_files.append(file_path)
                            print(f"   âœ… å·²ä¿å­˜: {file_path}")
                            print(f"   å¤§å°: {len(image_data) / 1024:.2f} KB")
                        else:
                            print(f"   âš ï¸  æ–‡ä»¶å¤ªå°ï¼Œè·³è¿‡")
                    else:
                        print(f"   âŒ ä¸‹è½½å¤±è´¥")
                    
                    await asyncio.sleep(1)  # ç¤¼è²Œå»¶æ—¶
                    
                except Exception as e:
                    print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                    continue
            
            await browser.close()
        
        print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
        print(f"   æˆåŠŸä¸‹è½½: {len(downloaded_files)} å¼ å›¾ç‰‡")
        print(f"   ä¿å­˜ä½ç½®: /tmp/test_image_*.jpg")
        
        print("\n" + "="*60)
        print("âœ… æµ‹è¯• 2 å®Œæˆ")
        print("="*60 + "\n")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_full_crawler_pipeline():
    """
    æµ‹è¯•å®Œæ•´çš„çˆ¬è™«æµç¨‹ï¼ˆåŒ…æ‹¬ Supabase ä¸Šä¼ ï¼‰ã€‚
    
    æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦ Supabase é…ç½®æ­£ç¡®ã€‚
    """
    print("\n" + "="*60)
    print("æµ‹è¯• 3: å®Œæ•´çˆ¬è™«æµç¨‹ï¼ˆå« Supabase ä¸Šä¼ ï¼‰")
    print("="*60 + "\n")
    
    try:
        # åˆ›å»º Supabase å®¢æˆ·ç«¯
        print("ğŸ”— æ­£åœ¨è¿æ¥ Supabase...")
        supabase_client = SupabaseClient()
        print("âœ… Supabase è¿æ¥æˆåŠŸ")
        
        # åˆ›å»ºçˆ¬è™«
        print("\nğŸ•·ï¸  æ­£åœ¨åˆå§‹åŒ–çˆ¬è™«...")
        crawler = WebCrawler(supabase_client)
        print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = ["satellite image"]
        test_job_id = "test_crawler_job"
        
        print(f"\nğŸ” å¼€å§‹çˆ¬å–...")
        print(f"   æŸ¥è¯¢è¯: {test_queries}")
        print(f"   æ¯ä¸ªæŸ¥è¯¢é™åˆ¶: 2 å¼ å›¾ç‰‡")
        print(f"   æµ‹è¯• Job ID: {test_job_id}")
        print("="*60 + "\n")
        
        # æ‰§è¡Œçˆ¬å–
        image_urls = await crawler.crawl_images(
            queries=test_queries,
            limit=2,
            job_id=test_job_id
        )
        
        print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"   æˆåŠŸä¸Šä¼ : {len(image_urls)} å¼ å›¾ç‰‡åˆ° Supabase")
        
        if image_urls:
            print(f"\n   ä¸Šä¼ çš„å›¾ç‰‡ URL:")
            for i, url in enumerate(image_urls, 1):
                print(f"   {i}. {url}")
        
        print("\n" + "="*60)
        print("âœ… æµ‹è¯• 3 å®Œæˆ")
        print("="*60 + "\n")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nğŸ’¡ æç¤ºï¼š")
        print("   - ç¡®ä¿ .env æ–‡ä»¶ä¸­çš„ Supabase é…ç½®æ­£ç¡®")
        print("   - ç¡®ä¿å·²åœ¨ Supabase ä¸­åˆ›å»ºäº† 'images' bucket")
        print("   - ç¡®ä¿ Supabase bucket æ˜¯å…¬å¼€çš„")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°ã€‚"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          Playwright ç½‘ç»œçˆ¬è™«æµ‹è¯•å¥—ä»¶                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    print("æ­¤æµ‹è¯•å°†éªŒè¯ä»¥ä¸‹åŠŸèƒ½:")
    print("  1ï¸âƒ£  æµè§ˆå™¨å¯åŠ¨å’Œé¡µé¢å¯¼èˆª")
    print("  2ï¸âƒ£  å›¾ç‰‡æœç´¢å’Œä¿¡æ¯æå–")
    print("  3ï¸âƒ£  å›¾ç‰‡ä¸‹è½½")
    print("  4ï¸âƒ£  Supabase ä¸Šä¼ ï¼ˆå¯é€‰ï¼‰")
    print("\n" + "="*60 + "\n")
    
    results = []
    
    # æµ‹è¯• 1: åŸºç¡€çˆ¬è™«
    print("â–¶ï¸  è¿è¡Œæµ‹è¯• 1...")
    result1 = await test_basic_crawler()
    results.append(("åŸºç¡€çˆ¬è™«åŠŸèƒ½", result1))
    
    input("\nâ¸ï¸  æŒ‰ Enter ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
    
    # æµ‹è¯• 2: ä¸‹è½½åŠŸèƒ½
    print("\nâ–¶ï¸  è¿è¡Œæµ‹è¯• 2...")
    result2 = await test_crawler_with_download()
    results.append(("å›¾ç‰‡ä¸‹è½½åŠŸèƒ½", result2))
    
    # è¯¢é—®æ˜¯å¦æµ‹è¯• Supabase ä¸Šä¼ 
    print("\n" + "="*60)
    response = input("æ˜¯å¦æµ‹è¯• Supabase ä¸Šä¼ åŠŸèƒ½ï¼Ÿ(éœ€è¦æ­£ç¡®é…ç½® Supabase) [y/N]: ")
    
    if response.lower() in ['y', 'yes']:
        print("\nâ–¶ï¸  è¿è¡Œæµ‹è¯• 3...")
        result3 = await test_full_crawler_pipeline()
        results.append(("å®Œæ•´çˆ¬è™«æµç¨‹", result3))
    
    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60 + "\n")
    
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    total = len(results)
    passed = sum(1 for _, p in results if p)
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    print("="*60 + "\n")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

